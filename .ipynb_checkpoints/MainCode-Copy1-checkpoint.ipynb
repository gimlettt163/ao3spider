{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da3290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # 正则表达式，进行文字匹配`\n",
    "import urllib.request, urllib.error  # 制定URL，获取网页数据\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import xlsxwriter\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4056eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetInfo(html):\n",
    "    if html is None:\n",
    "        raise ValueError(\"HTML内容未定义\")\n",
    "    html = html.prettify()\n",
    "    soup = BeautifulSoup(html,\"html.parser\",from_encoding=\"utf-8\")\n",
    "    title_tag = soup.find('a')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"未找到标题\"\n",
    "\n",
    "    # 找到作者的<a>标签并提取文本\n",
    "    author_tag = soup.find('a', rel=\"author\")\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"未找到作者\"\n",
    "    comments_text= soup.find('dd', class_='comments')\n",
    "    if comments_text:\n",
    "        comments = comments_text.get_text(strip=True)\n",
    "    else:\n",
    "        comments=0 \n",
    "    bookmarks= soup.find('dd', class_='bookmarks')\n",
    "    if bookmarks:\n",
    "        bookmarks = bookmarks.get_text(strip=True)  \n",
    "    else:\n",
    "        bookmarks=0    \n",
    "    if soup.find('dd', class_='kudos'):\n",
    "        kudos=soup.find('dd', class_='kudos').get_text(strip=True)\n",
    "    else:\n",
    "        kudos=0\n",
    "    if soup.find('dd', class_='hits'):\n",
    "        hits=soup.find('dd', class_='hits').get_text(strip=True)\n",
    "    else:\n",
    "        hits=0  \n",
    "    stats = {\n",
    "    \"标题\":title,\n",
    "    \"作者\":author,\n",
    "    \"点赞\":kudos,\n",
    "    \"点击\":hits,\n",
    "    \"评论\":comments,   \n",
    "    \"收藏\":bookmarks   \n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b93de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SaveSheet(InfoDict,worksheet):\n",
    "#     for row, items in enumerate(data.values()):\n",
    "#         for col, item in enumerate(items):\n",
    "#             sheet.cell(row=row+1, column=col+1, value=item)\n",
    "#     # 保存Excel文件\n",
    "#     workbook.save(file_path)\n",
    "#     workbook = Workbook()\n",
    "#     sheet = workbook.active  # 激活sheet\n",
    "#     sheet.title = name  # 设置sheet名字\n",
    "#     sheet.append([\"标题\", \"作者\", \"点赞\",\"点击\", \"评论\", \"收藏\"])  # 插入标题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92aa7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askURL(url):\n",
    "    head = {  # 模拟浏览器头部信息，向豆瓣服务器发送消息\n",
    "        \"User-Agent\": \"Mozilla / 5.0(Windows NT 10.0; Win64; x64) AppleWebKit / 537.36(KHTML, like Gecko) Chrome / 80.0.3987.122  Safari / 537.36\"\n",
    "    }\n",
    "    # 用户代理，表示告诉豆瓣服务器，我们是什么类型的机器、浏览器（本质上是告诉浏览器，我们可以接收什么水平的文件内容）\n",
    "    request = urllib.request.Request(url, headers=head)\n",
    "    html = \"\"\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "        html = response.read().decode(\"utf-8\")\n",
    "    except urllib.error.URLError as e:\n",
    "        if hasattr(e, \"code\"):\n",
    "            print(e.code)\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(e.reason)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumsOfPages(url):\n",
    "    html=askURL(url)\n",
    "    # 定义HTML内容\n",
    "    soup = BeautifulSoup(html,\"html.parser\",from_encoding=\"utf-8\")\n",
    "    PagesInfo=soup.find('ol',class_=\"pagination actions\")\n",
    "    if PagesInfo is None:\n",
    "        return \"未找到分页信息\"\n",
    "    num=0\n",
    "    for item in PagesInfo.find_all('li'):\n",
    "        pages = item.find('a')\n",
    "        pages = item.get_text(strip=True) if item else \"未找到标题\"\n",
    "        if(pages.isdigit()):\n",
    "            num=pages\n",
    "    return int(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46dace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入查询角色：秦彻\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a2693\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\bs4\\__init__.py:228: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    chinese_str = input(\"请输入查询角色：\")\n",
    "#     file_path = 'output.xlsx'\n",
    "#     # 创建一个Excel文件\n",
    "#     workbook = xlsxwriter.Workbook(file_path)\n",
    "#     # 添加一个工作表\n",
    "#     sheet_name = chinese_str\n",
    "#     if sheet_name not in workbook.sheetnames:\n",
    "#         worksheet = workbook.create_sheet(title=sheet_name)\n",
    "#     else:\n",
    "#         worksheet = workbook[sheet_name]\n",
    "    file_path=chinese_str+'.csv'\n",
    "    \n",
    "    encoded_url = quote(chinese_str)\n",
    "    base_url='https://archiveofourown.org/works/search?'\n",
    "    query_url='work_search%5Bquery%5D='+encoded_url\n",
    "    url=f\"{base_url}&{query_url}\"\n",
    "    \n",
    "    \n",
    "    num=NumsOfPages(url)\n",
    "    num=int(num)\n",
    "    begin_url='https://archiveofourown.org/works/search?'\n",
    "    end_url='&work_search%5Bquery%5D='+encoded_url\n",
    "    \n",
    "    fieldnames = [\"标题\", \"作者\", \"点赞\",\"点击\", \"评论\", \"收藏\"]\n",
    "    \n",
    "    with open(file_path,'w', newline='',encoding='utf-8-sig') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)       \n",
    "        writer.writeheader()\n",
    "        for page in range(1, num + 1):\n",
    "            url = f\"{begin_url}page={page}&{end_url}\"\n",
    "            html = askURL(url)  # 保存获取到的网页源码\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for item in soup.find_all('li', role=\"article\"):\n",
    "                singleInfo=GetInfo(item)\n",
    "                writer.writerow(singleInfo)\n",
    "                \n",
    "        csv_file.close()\n",
    "\n",
    "\n",
    "#             worksheet[\"标题\"]=singleInfo[\"标题\"]\n",
    "#             worksheet[\"作者\"]=singleInfo[\"作者\"]\n",
    "#             worksheet[\"点赞\"]=singleInfo[\"点赞\"]\n",
    "#             worksheet[\"点击\"]=singleInfo[\"点击\"]\n",
    "#             worksheet[\"评论\"]=singleInfo[\"评论\"]\n",
    "#             worksheet[\"收藏\"]=singleInfo[\"收藏\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
